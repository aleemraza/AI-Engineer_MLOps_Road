ğŸš€ ML Model Deployment with FastAPI & Docker

This project demonstrates an end-to-end Machine Learning deployment pipeline, covering everything from model training to production-ready API serving and containerization.

It showcases real-world MLOps practices such as model serialization, API-based inference, logging, latency measurement, and Docker-based deployment.

ğŸ“Œ Project Overview

The workflow implemented in this project:

ML Model â†’ FastAPI Service â†’ Docker Container

Train a machine learning model using scikit-learn

Save the trained model as a reusable artifact

Serve the model using a FastAPI REST API

Add health checks, logging, and latency monitoring

Containerize the application using Docker

This setup is designed to be scalable and easily extendable to Kubernetes, GPU inference, and model versioning tools like MLflow.

ğŸ§  Features

âœ… Machine learning model training and serialization

âœ… REST API for real-time inference

âœ… Input validation using Pydantic

âœ… Health check endpoint for monitoring

âœ… Structured logging and inference latency tracking

âœ… Dockerized application for portability and reproducibility

ğŸ›  Tech Stack

Language: Python

Machine Learning: scikit-learn

API Framework: FastAPI

Validation: Pydantic

Server: Uvicorn

Containerization: Docker

ğŸ“‚ Project Structure
.
â”œâ”€â”€ app.py                 # FastAPI application
â”œâ”€â”€ train.py               # ML model training script
â”œâ”€â”€ model.pkl              # Trained ML model
â”œâ”€â”€ logging_config.py      # Logging configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # Docker configuration
â””â”€â”€ README.md

âš™ï¸ How It Works
1ï¸âƒ£ Train the Model
python train.py


This generates a trained model saved as model.pkl.

2ï¸âƒ£ Run the API Locally
uvicorn app:app --reload


Open Swagger UI:

http://127.0.0.1:8000/docs

3ï¸âƒ£ Make a Prediction

POST /predict

{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}


Response

{
  "prediction": 0,
  "latency": 0.0023
}

4ï¸âƒ£ Health Check
GET /health


Response:

{
  "status": "ok"
}

ğŸ³ Docker Deployment
Build Image
docker build -t ml-model-api .

Run Container
docker run -p 8000:8000 ml-model-api


Access:

http://localhost:8000/docs

ğŸ“ˆ Logging & Monitoring

Logs prediction results and inference latency

Ready for integration with monitoring tools such as Prometheus and Grafana

Health endpoint enables container orchestration readiness checks

ğŸ”® Future Enhancements

ğŸ” Model versioning with MLflow

â˜¸ï¸ Kubernetes deployment

âš¡ GPU-based inference

ğŸ” Authentication and rate limiting

ğŸ“Š Advanced monitoring and metrics

ğŸ‘¨â€ğŸ’» Author

Muhammad Aleem Raza
Machine Learning / MLOps Enthusiast